{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import pathlib\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_templates = list()\n",
    "for file_name in pathlib.Path(\"./batch_results/\").glob(\"*.jsonl\"):\n",
    "    print(file_name)\n",
    "    with open(file_name, \"r\") as f:\n",
    "        for line in f.readlines():\n",
    "            instance = json.loads(line)\n",
    "            custom_id = instance['custom_id']\n",
    "            problem_id = custom_id.split('-')[1]\n",
    "            template = instance['response']['body']['choices'][0]['message']['content']\n",
    "            if template not in test_templates:\n",
    "                test_templates.append(template)\n",
    "                with open(f\"cpp_templates/test_template_{problem_id}.cpp\", \"w\") as f:\n",
    "                    f.write(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "venus_ds = load_dataset(\"Elfsong/Venus\", \"cpp\", split='train')\n",
    "venus_dict = {}\n",
    "for instance in venus_ds:\n",
    "    venus_dict[int(instance['question_id'])] = instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leetcode_ds = load_dataset(\"Elfsong/leetcode_data\", split='train')\n",
    "leetcode_dict = {}\n",
    "for instance in leetcode_ds:\n",
    "    leetcode_dict[int(instance['problem_id'])] = instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "for template_name in pathlib.Path(\"./cpp_templates/\").glob(\"*.cpp\"):\n",
    "    if \"template\" not in template_name.stem: continue\n",
    "    problem_id = int(template_name.stem.split(\"_\")[2])\n",
    "\n",
    "    if problem_id not in leetcode_dict: continue\n",
    "    if problem_id not in venus_dict: continue\n",
    "\n",
    "    leetcode_instance = leetcode_dict[problem_id]\n",
    "    venus_instance = venus_dict[problem_id]\n",
    "\n",
    "    solutions = venus_instance['rt_list'] + venus_instance['mm_list']\n",
    "    if len(solutions) == 0: continue\n",
    "    solution_code = random.choice(solutions)['code']\n",
    "\n",
    "    test_cases = json.loads(leetcode_instance['test_cases'])\n",
    "    test_case_str = json.dumps(test_cases, separators=(',', ':'))\n",
    "    test_case_str_literal = f'R\"({test_case_str})\"'\n",
    "\n",
    "    template_code = open(template_name, \"r\").read()\n",
    "    template_code = template_code.replace(\"==Solution Code==\", solution_code)\n",
    "    template_code = template_code.replace(\"==Test Cases==\", test_case_str_literal)\n",
    "\n",
    "    with open(f\"cpp_templates/test_{problem_id}.cpp\", \"w\") as f:\n",
    "        f.write(template_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "ds = load_dataset(\"Elfsong/Venus_Model_Evaluation\", \"gpt_4o\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/mingzhe/Projects/Afterburner\")\n",
    "\n",
    "import json\n",
    "import utils\n",
    "from datasets import load_dataset, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Openai\n",
    "test_packs = list()\n",
    "with open(\"./batch_output/batch_6807b843a96c81909963f1215d3c414c_output.jsonl\", \"r\") as f:\n",
    "    for line in f.readlines():\n",
    "        instance = json.loads(line)\n",
    "        custom_id = instance['custom_id']\n",
    "        problem_id = custom_id.split('-')[1]\n",
    "        \n",
    "        try:\n",
    "            generated_solution = instance['response']['body']['choices'][0]['message']['content']\n",
    "            generated_solution = utils.extract_code_blocks(generated_solution)[0]['code']\n",
    "        except Exception as e:\n",
    "            print(f\"[-] Generation Error: {e}\")\n",
    "        finally:\n",
    "            test_packs.append({\"problem_id\": int(problem_id), \"solution\": generated_solution})\n",
    "\n",
    "ds = Dataset.from_list(test_packs)\n",
    "ds.push_to_hub(\"Elfsong/Venus_Model_Evaluation\", \"o4_mini\", private=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Claude\n",
    "test_packs = list()\n",
    "with open(\"./batch_output/msgbatch_01H6UHpUWZKnRRzBeFm7UPEz_results.jsonl\", \"r\") as f:\n",
    "    for line in f.readlines():\n",
    "        instance = json.loads(line)\n",
    "        custom_id = instance['custom_id']\n",
    "        problem_id = custom_id.split('-')[1]\n",
    "        \n",
    "        try:\n",
    "            generated_solution = instance['result']['message']['content'][0][\"text\"]\n",
    "            generated_solution = utils.extract_code_blocks(generated_solution)[0]['code']\n",
    "        except Exception as e:\n",
    "            print(f\"[-] Generation Error: {e}\")\n",
    "        finally:\n",
    "            test_packs.append({\"problem_id\": int(problem_id), \"solution\": generated_solution})\n",
    "\n",
    "ds = Dataset.from_list(test_packs)\n",
    "ds.push_to_hub(\"Elfsong/Venus_Model_Evaluation\", \"deepseek_v3_memory_claude_3_7_sonnet\", private=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "\n",
    "client = genai.Client(api_key=\"AIzaSyApfIZvktjh59jpVBNm5JSDM2rOJq3-2ac\")\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash\", contents=\"Explain how AI works in a few words\"\n",
    ")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "AFTERBURNER_GENERATION_TEMPLATE = \"\"\"\n",
    "## Instructions\n",
    "You are an expert competitive programmer who excels at solving algorithm problems in multiple programming languages.\n",
    "Your task is to implement a solution to the following problem in {target_lang}.\n",
    "\n",
    "## Problem Description\n",
    "{problem_description}\n",
    "\n",
    "## Original Solution\n",
    "```python\n",
    "{original_solution}\n",
    "```\n",
    "\n",
    "## Original Performance\n",
    "Passed: {original_passed} / Time: {original_time} / Memory: {original_memory} / Integral: {original_integral}\n",
    "\n",
    "## Output Formats\n",
    "- Fix the original solution if it was not passed. Optimize the {efficiency_instruction} performance if the original solution was passed.\n",
    "- EXCLUDE ALL explanations, code comments, import/package/library statements, additional classes or functions outside of the starter code scope, or starting code like `if __name__ == \"__main__\":` or `func main()` or `package main` or `using namespace std;`.\n",
    "- Start your response with your thinking process within <thinking>...</thinking> tags, and provide the complete solution code with <solution>...</solution> tags.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = AFTERBURNER_GENERATION_TEMPLATE.format(\n",
    "    target_lang=\"python\", \n",
    "    problem_description=\"You are given two strings `word1` and `word2`. Merge the strings by adding letters in alternating order, starting with `word1`. If a string is longer than the other, append the additional letters onto the end of the merged string.\\n\\n\\nReturn *the merged string.*\", \n",
    "    original_solution=\"class Solution:\\n    def mergeAlternately(self, word1: str, word2: str) -> str:\\n        i=j=k=0\\n        str1 = \\\"\\\"\\n\\n        while i<len(word1) and j<len(word2):\\n            str1+=word1[i]+word2[j]\\n            i+=1\\n            j+=1\\n\\n        if i<len(word1):\\n            str1+=word1[i:]\\n        if j <len(word2):\\n            str1+=word2[j:]\\n        return (str1)\", \n",
    "    original_passed=True, \n",
    "    original_time=580.404275, \n",
    "    original_memory=23404.0, \n",
    "    original_integral=296272.0, \n",
    "    efficiency_instruction=\"time-efficient\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "client = genai.Client(api_key=\"AIzaSyApfIZvktjh59jpVBNm5JSDM2rOJq3-2ac\")\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-pro-preview-03-25\",\n",
    "    contents=prompt,\n",
    "    config=types.GenerateContentConfig(\n",
    "        thinking_config=types.ThinkingConfig(thinking_budget=2048)\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=\"https://api.studio.nebius.com/v1/\",\n",
    "    api_key=\"eyJhbGciOiJIUzI1NiIsImtpZCI6IlV6SXJWd1h0dnprLVRvdzlLZWstc0M1akptWXBvX1VaVkxUZlpnMDRlOFUiLCJ0eXAiOiJKV1QifQ.eyJzdWIiOiJnb29nbGUtb2F1dGgyfDEwMjM1NjEzMDM1MDE1MTM1ODEwMiIsInNjb3BlIjoib3BlbmlkIG9mZmxpbmVfYWNjZXNzIiwiaXNzIjoiYXBpX2tleV9pc3N1ZXIiLCJhdWQiOlsiaHR0cHM6Ly9uZWJpdXMtaW5mZXJlbmNlLmV1LmF1dGgwLmNvbS9hcGkvdjIvIl0sImV4cCI6MTkwMzMzNjg3NiwidXVpZCI6ImE3MGZhNGUwLTg0NGYtNGFiMS1hNjE4LWM1ZmZmMjJlOGI1YiIsIm5hbWUiOiJDb2RlIiwiZXhwaXJlc19hdCI6IjIwMzAtMDQtMjVUMDg6NDE6MTYrMDAwMCJ9.mv-eNeo2FzIpRHAsdQWfSj81ZbmfP9hUyPmfGqyVRRo\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_requests = client.files.create(\n",
    "    file=open(\"/home/mingzhe/Projects/Afterburner/evaluation/venus/batch_input/_integral_qwen_32b_batchinput.jsonl\", \"rb\"),\n",
    "    purpose=\"batch\"\n",
    ")\n",
    "print(batch_requests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Batch(id='batch_e99adb9c-d359-47e5-978a-7e820c0a2698', completion_window='24h', created_at=1745697027, endpoint='/v1/chat/completions', input_file_id='file-d5cf6f24-ba7d-4da4-b265-ba7a8a2f8b27', object='batch', status='validating', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=None, expired_at=None, expires_at=None, failed_at=None, finalizing_at=None, in_progress_at=None, metadata={'description': 'Asynchronous job'}, output_file_id=None, request_counts=BatchRequestCounts(completed=None, failed=None, total=None))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_request = client.batches.create(\n",
    "    input_file_id=batch_requests.id,\n",
    "    endpoint=\"/v1/chat/completions\",\n",
    "    completion_window=\"24h\",\n",
    "    metadata={\n",
    "        \"description\": \"Asynchronous job\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch(id='batch_e99adb9c-d359-47e5-978a-7e820c0a2698', completion_window='24h', created_at=1745697027, endpoint='/v1/chat/completions', input_file_id='file-d5cf6f24-ba7d-4da4-b265-ba7a8a2f8b27', object='batch', status='done', cancelled_at=None, cancelling_at=None, completed_at=1745700933, error_file_id=None, errors=None, expired_at=None, expires_at=None, failed_at=None, finalizing_at=1745700933, in_progress_at=1745697031, metadata={'description': 'Asynchronous job'}, output_file_id='39b7c2c6-1bc0-4b5b-9f4e-1bc7e63ed238', request_counts=BatchRequestCounts(completed=300, failed=0, total=300))\n"
     ]
    }
   ],
   "source": [
    "status = client.batches.retrieve(\"batch_e99adb9c-d359-47e5-978a-7e820c0a2698\")\n",
    "print(status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_result = client.files.content(status.output_file_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/mingzhe/Projects/Afterburner\")\n",
    "\n",
    "import json\n",
    "import utils\n",
    "from datasets import load_dataset, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-] Generation Error: list index out of range\n",
      "[-] Generation Error: list index out of range\n",
      "[-] Generation Error: list index out of range\n",
      "[-] Generation Error: list index out of range\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04c5fe845c0142a7a25268cedc4dc935",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "287ebbe5198145cb99583cd849aa9e8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/Elfsong/Venus_Model_Evaluation/commit/385644fdc6cb52cbd575866b9da94dc6834202b7', commit_message='Upload dataset', commit_description='', oid='385644fdc6cb52cbd575866b9da94dc6834202b7', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/Elfsong/Venus_Model_Evaluation', endpoint='https://huggingface.co', repo_type='dataset', repo_id='Elfsong/Venus_Model_Evaluation'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_packs = list()\n",
    "\n",
    "for line in batch_result.iter_lines():\n",
    "    instance = json.loads(line)\n",
    "    custom_id = instance['custom_id']\n",
    "    problem_id = custom_id.split('-')[1]\n",
    "    \n",
    "    try:\n",
    "        generated_solution = instance['response']['choices'][0]['message']['content']\n",
    "        generated_solution = utils.extract_code_blocks(generated_solution)[-1]['code']\n",
    "    except Exception as e:\n",
    "        print(f\"[-] Generation Error: {e}\")\n",
    "    finally:\n",
    "        test_packs.append({\"problem_id\": int(problem_id), \"solution\": generated_solution})\n",
    "\n",
    "ds = Dataset.from_list(test_packs)\n",
    "ds.push_to_hub(\"Elfsong/Venus_Model_Evaluation\", \"qwq_32b\", private=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Login using e.g. `huggingface-cli login` to access this dataset\n",
    "ds = load_dataset(\"Elfsong/Venus_Model_Evaluation\", \"qwq_32b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'problem_id': 2017,\n",
       " 'solution': \"class Solution:\\n    def minFlips(self, s: str) -> int:\\n        n = len(s)\\n        if n == 0:\\n            return 0\\n        \\n        a0 = [0] * n\\n        a1 = [0] * n\\n        for i in range(n):\\n            if s[i] == '0':\\n                a0[i] = 0\\n                a1[i] = 1\\n            else:\\n                a0[i] = 1\\n                a1[i] = 0\\n        \\n        # Compute the four term arrays\\n        term0_even = [a0[i] if (i % 2 == 0) else a1[i] for i in range(n)]\\n        term0_odd = [a0[i] if (i % 2 == 1) else a1[i] for i in range(n)]\\n        term1_even = [a1[i] if (i % 2 == 0) else a0[i] for i in range(n)]\\n        term1_odd = [a1[i] if (i % 2 == 1) else a0[i] for i in range(n)]\\n        \\n        # Create doubled arrays\\n        term0_even_doubled = term0_even + term0_even\\n        term0_odd_doubled = term0_odd + term0_odd\\n        term1_even_doubled = term1_even + term1_even\\n        term1_odd_doubled = term1_odd + term1_odd\\n        \\n        # Compute prefix sums\\n        prefix0_even = [0] * (2 * n + 1)\\n        for i in range(2 * n):\\n            prefix0_even[i + 1] = prefix0_even[i] + term0_even_doubled[i]\\n        \\n        prefix0_odd = [0] * (2 * n + 1)\\n        for i in range(2 * n):\\n            prefix0_odd[i + 1] = prefix0_odd[i] + term0_odd_doubled[i]\\n        \\n        prefix1_even = [0] * (2 * n + 1)\\n        for i in range(2 * n):\\n            prefix1_even[i + 1] = prefix1_even[i] + term1_even_doubled[i]\\n        \\n        prefix1_odd = [0] * (2 * n + 1)\\n        for i in range(2 * n):\\n            prefix1_odd[i + 1] = prefix1_odd[i] + term1_odd_doubled[i]\\n        \\n        min_flips = float('inf')\\n        for i in range(n):\\n            if i % 2 == 0:\\n                sum0 = prefix0_even[i + n] - prefix0_even[i]\\n                sum1 = prefix1_even[i + n] - prefix1_even[i]\\n            else:\\n                sum0 = prefix0_odd[i + n] - prefix0_odd[i]\\n                sum1 = prefix1_odd[i + n] - prefix1_odd[i]\\n            \\n            current_min = min(sum0, sum1)\\n            if current_min < min_flips:\\n                min_flips = current_min\\n        \\n        return min_flips\"}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "verl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
